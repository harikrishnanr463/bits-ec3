<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Unit Notes | EC3 Prep Hub â€” BITS WILP AIML</title>
  <meta name="description" content="Concise unit-wise study notes for MFML, ISM, ML and DNN â€” BITS WILP MTech AIML EC3 preparation." />
  <link rel="stylesheet" href="../css/style.css" />
</head>
<body>
<nav class="navbar">
  <a href="../index.html" class="nav-brand"><div class="logo-icon">ğŸ“</div><span>EC3 Prep Hub</span></a>
  <ul class="nav-links" id="navLinks">
    <li><a href="../index.html">Home</a></li>
    <li><a href="syllabus.html">Syllabus</a></li>
    <li><a href="notes.html" class="active">Notes</a></li>
    <li><a href="pyq.html">PYQs</a></li>
    <li><a href="mocktest.html">Mock Tests</a></li>
    <li><a href="tips.html">Exam Tips</a></li>
    <li><a href="revision.html">Quick Revision</a></li>
    <li><a href="confidence.html">Confidence Booster</a></li>
  </ul>
  <button class="nav-toggle" id="navToggle"><span></span><span></span><span></span></button>
</nav>

<section class="hero" style="padding:3rem 1.5rem 2rem">
  <div class="hero-badge">ğŸ“š Unit-Wise Study Notes</div>
  <h1>Clear Notes for<br><span class="highlight">Every Key Concept</span></h1>
  <p class="tagline">Original, concise summaries of every exam-relevant topic. Use these alongside your textbooks.</p>
  <p class="micro-copy">ğŸ“– These notes supplement â€” don't replace â€” your textbooks. Always go deeper on topics that appear in PYQs.</p>
</section>

<div class="section">
  <!-- Subject Tabs -->
  <div class="subject-tabs">
    <button class="tab-btn active" data-tab="tab-mfml">âˆ‘ MFML</button>
    <button class="tab-btn" data-tab="tab-ism">ğŸ“Š ISM</button>
    <button class="tab-btn" data-tab="tab-ml">ğŸ¤– ML</button>
    <button class="tab-btn" data-tab="tab-dnn">ğŸ§  DNN</button>
  </div>

  <!-- MFML NOTES -->
  <div id="tab-mfml" class="tab-content active" id="mfml">
    <div style="margin-bottom:1.5rem;padding:1.25rem;background:var(--lavender-soft);border-radius:var(--radius-sm)">
      <strong style="color:var(--accent-purple)">ğŸ’¡ MFML Strategy:</strong>
      <span style="color:var(--text-mid);font-size:0.9rem"> Start from Week 1. Linear algebra is foundational for all other subjects. Don't skip proofs â€” understanding why is what the exam tests.</span>
    </div>
    <div class="accordion">

      <div class="accordion-item">
        <button class="accordion-header">Unit 1: Linear Algebra Fundamentals <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Vectors:</strong> An ordered list of numbers. A vector v âˆˆ â„â¿ has n components. Key operations: addition, scalar multiplication, dot product.</p>
          <p style="margin-top:1rem"><strong>Dot Product:</strong> uÂ·v = Î£áµ¢ uáµ¢váµ¢ = |u||v|cos(Î¸). Used to measure similarity/angle between vectors.</p>
          <p style="margin-top:1rem"><strong>Linear Independence:</strong> Vectors vâ‚,...,vâ‚™ are linearly independent if no vector can be written as a linear combination of the others. Equivalently, câ‚vâ‚ + ... + câ‚™vâ‚™ = 0 implies all cáµ¢ = 0.</p>
          <p style="margin-top:1rem"><strong>Span:</strong> The set of all linear combinations of a set of vectors. Forms a vector subspace.</p>
          <p style="margin-top:1rem"><strong>Basis:</strong> A set of linearly independent vectors that spans the space. The number of basis vectors = dimension of the space.</p>
          <p style="margin-top:1rem"><strong>Rank:</strong> The maximum number of linearly independent columns (= rows) of a matrix. rank(A) â‰¤ min(m, n) for A âˆˆ â„áµË£â¿.</p>
          <p style="margin-top:1rem"><strong>Null Space:</strong> {x : Ax = 0}. Rank-Nullity theorem: rank(A) + nullity(A) = n (number of columns).</p>
          <p style="margin-top:1rem"><strong>Key Formulas:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>det(AB) = det(A)Â·det(B)</li>
            <li>(AB)â»Â¹ = Bâ»Â¹Aâ»Â¹</li>
            <li>(AB)áµ€ = Báµ€Aáµ€</li>
            <li>tr(AB) = tr(BA)</li>
          </ul>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 2: Inner Products &amp; Orthogonality <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Inner Product:</strong> A generalization of dot product satisfying symmetry, linearity, and positive definiteness: âŸ¨x,xâŸ© â‰¥ 0 with equality iff x=0.</p>
          <p style="margin-top:1rem"><strong>Norm:</strong> ||x|| = âˆšâŸ¨x,xâŸ©. L1 norm: Î£|xáµ¢|. L2 norm: âˆš(Î£xáµ¢Â²). Lâˆ norm: max|xáµ¢|.</p>
          <p style="margin-top:1rem"><strong>Orthogonality:</strong> uâŠ¥v iff âŸ¨u,vâŸ© = 0. Orthogonal vectors are "perpendicular" in the inner product space.</p>
          <p style="margin-top:1rem"><strong>Orthogonal Projection:</strong> The projection of x onto subspace U spanned by {bâ‚,...,bâ‚–}: Ï€(x) = Î£áµ¢ [âŸ¨x,báµ¢âŸ©/âŸ¨báµ¢,báµ¢âŸ©]báµ¢. If B is orthonormal: Ï€(x) = BBáµ€x.</p>
          <p style="margin-top:1rem"><strong>Gram-Schmidt:</strong> Process to orthogonalize a set of vectors. Starting with vâ‚,...,vâ‚™: eâ‚ = vâ‚/||vâ‚||, then subtract projections iteratively. Used for QR decomposition.</p>
          <p style="margin-top:1rem"><strong>Cauchy-Schwarz Inequality:</strong> |âŸ¨u,vâŸ©| â‰¤ ||u||Â·||v||. Equality holds iff u and v are collinear.</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 3: Eigenvalues, Eigenvectors &amp; SVD <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Eigenvalue Equation:</strong> Av = Î»v. Non-trivial solutions exist when det(A - Î»I) = 0 (characteristic equation).</p>
          <p style="margin-top:1rem"><strong>Properties:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>tr(A) = Î£ eigenvalues</li>
            <li>det(A) = Î  eigenvalues</li>
            <li>Symmetric matrices: all eigenvalues real</li>
            <li>Positive definite: all eigenvalues &gt; 0</li>
          </ul>
          <p style="margin-top:1rem"><strong>Diagonalization:</strong> A = PDPâ»Â¹ where P has eigenvectors as columns and D is diagonal with eigenvalues. Requires n linearly independent eigenvectors.</p>
          <p style="margin-top:1rem"><strong>Spectral Theorem:</strong> Any real symmetric matrix A can be written as A = QÎ›Qáµ€ where Q is orthogonal and Î› is diagonal. (Q has orthonormal eigenvectors.)</p>
          <p style="margin-top:1rem"><strong>SVD:</strong> For any A âˆˆ â„áµË£â¿: A = UÎ£Váµ€</p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>U âˆˆ â„áµË£áµ: left singular vectors (eigenvectors of AAáµ€)</li>
            <li>V âˆˆ â„â¿Ë£â¿: right singular vectors (eigenvectors of Aáµ€A)</li>
            <li>Î£: diagonal matrix of singular values Ïƒáµ¢ = âˆš(eigenvalue of Aáµ€A) â‰¥ 0</li>
          </ul>
          <p style="margin-top:1rem"><strong>SVD Applications:</strong> PCA (take top-k singular vectors), low-rank approximation, least squares, image compression.</p>
          <p style="margin-top:1rem"><strong>Low-rank Approximation:</strong> Best rank-k approximation: Aâ‚– = Î£áµ¢â‚Œâ‚áµ Ïƒáµ¢uáµ¢váµ¢áµ€ (Eckart-Young theorem).</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 4: Gradient, Jacobian &amp; Hessian <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Partial Derivative:</strong> âˆ‚f/âˆ‚xáµ¢ â€” derivative of f with respect to xáµ¢, treating all other variables as constants.</p>
          <p style="margin-top:1rem"><strong>Gradient:</strong> âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€. Points in the direction of steepest ascent. Used in gradient descent: Î¸ â† Î¸ - Î±âˆ‡L(Î¸).</p>
          <p style="margin-top:1rem"><strong>Jacobian:</strong> For f: â„â¿ â†’ â„áµ, the Jacobian J âˆˆ â„áµË£â¿ has Jáµ¢â±¼ = âˆ‚fáµ¢/âˆ‚xâ±¼. Generalizes the gradient to vector-valued functions.</p>
          <p style="margin-top:1rem"><strong>Hessian:</strong> H = âˆ‡Â²f. The matrix of second-order partial derivatives: Háµ¢â±¼ = âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼. Used to classify critical points (positive definite â†’ local minimum).</p>
          <p style="margin-top:1rem"><strong>Chain Rule (multivariate):</strong> If z = f(g(x)), then âˆ‚z/âˆ‚x = (âˆ‚z/âˆ‚g)(âˆ‚g/âˆ‚x). This is the mathematical foundation of backpropagation in neural networks.</p>
          <p style="margin-top:1rem"><strong>Taylor Expansion:</strong> f(x+Î´) â‰ˆ f(x) + âˆ‡f(x)áµ€Î´ + Â½Î´áµ€H(x)Î´. The Hessian gives the second-order correction.</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 5 &amp; 6: Optimization â€” Gradient Descent &amp; KKT <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Convex Function:</strong> f is convex if f(Î»x + (1-Î»)y) â‰¤ Î»f(x) + (1-Î»)f(y). For convex f, any local minimum is a global minimum.</p>
          <p style="margin-top:1rem"><strong>Gradient Descent:</strong> Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î±âˆ‡f(Î¸â‚œ). Converges to global minimum for convex f with appropriate Î±. Step size Î± (learning rate) must be chosen carefully.</p>
          <p style="margin-top:1rem"><strong>Convergence Condition:</strong> If âˆ‡f is L-Lipschitz, gradient descent with Î± â‰¤ 1/L converges. Rate: f(Î¸â‚œ) - f* â‰¤ O(1/t) for convex f.</p>
          <p style="margin-top:1rem"><strong>Lagrange Multipliers:</strong> To minimize f(x) subject to g(x)=0: form L(x,Î») = f(x) - Î»g(x). Set âˆ‡L = 0. The Î» values are Lagrange multipliers.</p>
          <p style="margin-top:1rem"><strong>KKT Conditions:</strong> For inequality constraints gáµ¢(x) â‰¤ 0:</p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Stationarity: âˆ‡f(x*) + Î£Î»áµ¢âˆ‡gáµ¢(x*) = 0</li>
            <li>Primal feasibility: gáµ¢(x*) â‰¤ 0</li>
            <li>Dual feasibility: Î»áµ¢ â‰¥ 0</li>
            <li>Complementary slackness: Î»áµ¢gáµ¢(x*) = 0</li>
          </ul>
        </div>
      </div>

    </div><!-- end accordion -->

    <div class="floating-quote" style="margin-top:2rem">
      <blockquote>"The solutions manual for MML is a must-have. Work through problems, not around them."</blockquote>
      <cite>â€” BITS WILP Seniors</cite>
    </div>
  </div>

  <!-- ISM NOTES -->
  <div id="tab-ism" class="tab-content" id="ism">
    <div style="margin-bottom:1.5rem;padding:1.25rem;background:var(--mint-soft);border-radius:var(--radius-sm)">
      <strong style="color:var(--accent-green)">ğŸ’¡ ISM Strategy:</strong>
      <span style="color:var(--text-mid);font-size:0.9rem"> ISM is the most scoring subject. Solve Devore problems daily. The exam is 80% numerical. The more you calculate, the faster and more accurate you get.</span>
    </div>
    <div class="accordion">

      <div class="accordion-item">
        <button class="accordion-header">Unit 1: Probability Foundations &amp; Bayes' Theorem <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Probability Axioms:</strong> P(Î©)=1, P(A)â‰¥0 for all A, P(AâˆªB) = P(A)+P(B) for disjoint A,B.</p>
          <p style="margin-top:1rem"><strong>Conditional Probability:</strong> P(A|B) = P(Aâˆ©B)/P(B). The probability of A given that B has occurred.</p>
          <p style="margin-top:1rem"><strong>Independence:</strong> A and B are independent iff P(Aâˆ©B) = P(A)Â·P(B), equivalently P(A|B) = P(A).</p>
          <p style="margin-top:1rem"><strong>Total Probability:</strong> P(B) = Î£áµ¢ P(B|Aáµ¢)P(Aáµ¢) for a partition {Aáµ¢} of the sample space.</p>
          <p style="margin-top:1rem"><strong>Bayes' Theorem:</strong> P(A|B) = P(B|A)P(A)/P(B)</p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>P(A) = Prior probability</li>
            <li>P(B|A) = Likelihood</li>
            <li>P(B) = Marginal (Evidence) = Î£ P(B|Aáµ¢)P(Aáµ¢)</li>
            <li>P(A|B) = Posterior probability</li>
          </ul>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 2 &amp; 3: Distributions â€” The Key Table <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <table>
            <thead><tr><th>Distribution</th><th>Mean</th><th>Variance</th><th>PMF/PDF</th></tr></thead>
            <tbody>
              <tr><td><strong>Bernoulli(p)</strong></td><td>p</td><td>p(1-p)</td><td>P(X=1)=p, P(X=0)=1-p</td></tr>
              <tr><td><strong>Binomial(n,p)</strong></td><td>np</td><td>np(1-p)</td><td>C(n,k)páµ(1-p)â¿â»áµ</td></tr>
              <tr><td><strong>Poisson(Î»)</strong></td><td>Î»</td><td>Î»</td><td>eâ»Î»Î»áµ/k!</td></tr>
              <tr><td><strong>Geometric(p)</strong></td><td>1/p</td><td>(1-p)/pÂ²</td><td>(1-p)áµâ»Â¹p</td></tr>
              <tr><td><strong>Uniform(a,b)</strong></td><td>(a+b)/2</td><td>(b-a)Â²/12</td><td>1/(b-a) for xâˆˆ[a,b]</td></tr>
              <tr><td><strong>Exponential(Î»)</strong></td><td>1/Î»</td><td>1/Î»Â²</td><td>Î»eâ»Î»x for xâ‰¥0</td></tr>
              <tr><td><strong>Normal(Î¼,ÏƒÂ²)</strong></td><td>Î¼</td><td>ÏƒÂ²</td><td>(1/Ïƒâˆš2Ï€)exp(-(x-Î¼)Â²/2ÏƒÂ²)</td></tr>
            </tbody>
          </table>
          <p style="margin-top:1rem;color:var(--text-mid);font-size:0.9rem">ğŸ’¡ Memorize this table. These distributions appear in almost every exam and the parameters come up in every type of calculation.</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 4 &amp; 5: CLT, Confidence Intervals &amp; MLE <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Central Limit Theorem:</strong> For i.i.d. Xâ‚,...,Xâ‚™ with mean Î¼, variance ÏƒÂ²: XÌ„ ~ N(Î¼, ÏƒÂ²/n) as nâ†’âˆ. Standard error = Ïƒ/âˆšn.</p>
          <p style="margin-top:1rem"><strong>Standardization:</strong> Z = (XÌ„ - Î¼)/(Ïƒ/âˆšn) ~ N(0,1). Used to compute probabilities about sample means.</p>
          <p style="margin-top:1rem"><strong>Confidence Intervals (Ïƒ known):</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>90% CI: XÌ„ Â± 1.645(Ïƒ/âˆšn)</li>
            <li>95% CI: XÌ„ Â± 1.96(Ïƒ/âˆšn) â† Most common!</li>
            <li>99% CI: XÌ„ Â± 2.576(Ïƒ/âˆšn)</li>
          </ul>
          <p style="margin-top:1rem"><strong>t-distribution (Ïƒ unknown):</strong> Use t critical value with n-1 degrees of freedom. CI: XÌ„ Â± t*(s/âˆšn).</p>
          <p style="margin-top:1rem"><strong>MLE (Maximum Likelihood Estimation):</strong> Find Î¸ that maximizes L(Î¸) = Î  f(xáµ¢|Î¸). Equivalently, maximize â„“(Î¸) = Î£ log f(xáµ¢|Î¸).</p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Exponential(Î»): Î»Ì‚ = 1/xÌ„</li>
            <li>Normal(Î¼,ÏƒÂ²): Î¼Ì‚ = xÌ„, ÏƒÌ‚Â² = (1/n)Î£(xáµ¢-xÌ„)Â²</li>
            <li>Binomial(n,p): pÌ‚ = xÌ„/n</li>
            <li>Poisson(Î»): Î»Ì‚ = xÌ„</li>
          </ul>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Unit 6: Hypothesis Testing â€” Complete Framework <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Framework:</strong> Hâ‚€ (null) vs Hâ‚ (alternative). Choose Î± (significance level, typically 0.05).</p>
          <p style="margin-top:1rem"><strong>Error Types:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Type I Error (Î±): Reject Hâ‚€ when it's true (false positive)</li>
            <li>Type II Error (Î²): Fail to reject Hâ‚€ when it's false (false negative)</li>
            <li>Power = 1 - Î²: Probability of correctly rejecting false Hâ‚€</li>
          </ul>
          <p style="margin-top:1rem"><strong>p-value:</strong> P(observing result as extreme | Hâ‚€ true). Reject Hâ‚€ if p-value &lt; Î±.</p>
          <p style="margin-top:1rem"><strong>Test Selection Guide:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>One sample, Ïƒ known â†’ z-test</li>
            <li>One sample, Ïƒ unknown â†’ t-test (df = n-1)</li>
            <li>Two samples â†’ two-sample t-test</li>
            <li>Proportions â†’ z-test for proportions</li>
            <li>Categorical data â†’ Chi-square test</li>
            <li>Multiple group means â†’ ANOVA (F-test)</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <!-- ML NOTES -->
  <div id="tab-ml" class="tab-content" id="ml">
    <div style="margin-bottom:1.5rem;padding:1.25rem;background:var(--blue-soft);border-radius:var(--radius-sm)">
      <strong style="color:var(--accent-blue)">ğŸ’¡ ML Strategy:</strong>
      <span style="color:var(--text-mid);font-size:0.9rem"> Run GÃ©ron's notebooks first, then read theory. Seeing output makes math click. Know scikit-learn API well for implementation questions.</span>
    </div>
    <div class="accordion">

      <div class="accordion-item">
        <button class="accordion-header">Bias-Variance Tradeoff &amp; Regularization <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Total Error:</strong> E[(y - Å·)Â²] = BiasÂ² + Variance + Irreducible Noise</p>
          <p style="margin-top:1rem"><strong>Bias:</strong> Error from wrong assumptions in the model (model is too simple). High bias = underfitting.</p>
          <p style="margin-top:1rem"><strong>Variance:</strong> Error from sensitivity to fluctuations in training data (model is too complex). High variance = overfitting.</p>
          <p style="margin-top:1rem"><strong>Regularization:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li><strong>L2 (Ridge):</strong> Adds Î»Î£wáµ¢Â² to loss. Shrinks weights toward 0 but rarely to exactly 0. Good when all features contribute.</li>
            <li><strong>L1 (Lasso):</strong> Adds Î»Î£|wáµ¢| to loss. Produces sparse solutions (some weights = 0). Good for feature selection.</li>
            <li><strong>ElasticNet:</strong> Combines L1 and L2. Best of both worlds.</li>
          </ul>
          <p style="margin-top:1rem"><strong>L2 closed form (Ridge):</strong> w = (Xáµ€X + Î»I)â»Â¹Xáµ€y. Note: adding Î»I ensures invertibility even when Xáµ€X is singular!</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Support Vector Machines â€” Complete Theory <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Core Idea:</strong> Find the hyperplane wáµ€x + b = 0 that maximizes the margin (2/||w||) between classes.</p>
          <p style="margin-top:1rem"><strong>Optimization:</strong> Minimize Â½||w||Â² subject to yáµ¢(wáµ€xáµ¢+b) â‰¥ 1 for all i. This is a QP problem with a unique global minimum.</p>
          <p style="margin-top:1rem"><strong>Support Vectors:</strong> Training points that lie exactly on the margin boundary (yáµ¢(wáµ€xáµ¢+b) = 1). Only these determine the decision boundary.</p>
          <p style="margin-top:1rem"><strong>Soft Margin SVM:</strong> Introduces slack variables Î¾áµ¢ â‰¥ 0: minimize Â½||w||Â² + CÎ£Î¾áµ¢. Parameter C controls regularization: small C â†’ wider margin (more tolerance), large C â†’ harder margin.</p>
          <p style="margin-top:1rem"><strong>Kernel Trick:</strong> Replace xáµ¢Â·xâ±¼ with K(xáµ¢,xâ±¼) = Ï†(xáµ¢)Â·Ï†(xâ±¼) without computing Ï† explicitly.</p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Linear: K(x,z) = xáµ€z</li>
            <li>Polynomial: K(x,z) = (xáµ€z + c)áµˆ</li>
            <li>RBF/Gaussian: K(x,z) = exp(-Î³||x-z||Â²) â† most popular</li>
          </ul>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Decision Trees, Random Forests &amp; Boosting <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Decision Tree:</strong> Recursively partitions feature space. Split criterion: Information Gain (ID3), Gini Impurity (CART), Variance Reduction (regression).</p>
          <p style="margin-top:1rem"><strong>Gini Impurity:</strong> G = 1 - Î£pâ‚–Â². Perfect split G=0, worst G=0.5 (binary). Lower is better.</p>
          <p style="margin-top:1rem"><strong>Information Gain:</strong> IG = H(parent) - Î£(|child|/|parent|)H(child). H(S) = -Î£pâ‚–logâ‚‚pâ‚– (entropy).</p>
          <p style="margin-top:1rem"><strong>Random Forest:</strong> Bagging (bootstrap sampling) + random feature subsets at each split. Reduces variance without increasing bias. Feature importance = average decrease in impurity.</p>
          <p style="margin-top:1rem"><strong>Gradient Boosting:</strong> Ensemble where each tree corricts residuals of the previous. Loss function L(y,Å·), each new tree fits -âˆ‚L/âˆ‚Å·. XGBoost adds regularization and second-order gradients.</p>
          <p style="margin-top:1rem"><strong>Bagging vs Boosting:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Bagging: parallel, reduces variance, good for high-variance models</li>
            <li>Boosting: sequential, reduces bias, good for weak learners</li>
          </ul>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Clustering: K-Means &amp; EM Algorithm <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>K-Means Algorithm:</strong></p>
          <ol style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Initialize K centroids randomly</li>
            <li>Assign each point to nearest centroid</li>
            <li>Recompute centroids as mean of assigned points</li>
            <li>Repeat until convergence</li>
          </ol>
          <p style="margin-top:1rem"><strong>K-Means objective:</strong> Minimize J = Î£áµ¢ Î£â‚– ráµ¢â‚– ||xáµ¢ - Î¼â‚–||Â². Convergence guaranteed but to local minimum only.</p>
          <p style="margin-top:1rem"><strong>Choosing K:</strong> Elbow method (plot J vs K), Silhouette score, BIC/AIC for GMMs.</p>
          <p style="margin-top:1rem"><strong>EM Algorithm (for GMMs):</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>E-step: Compute responsibilities ráµ¢â‚– = P(component k | xáµ¢, Î¸)</li>
            <li>M-step: Update parameters Ï€, Î¼, Î£ using responsibilities</li>
            <li>Guaranteed to increase log-likelihood at each iteration</li>
          </ul>
        </div>
      </div>

    </div>
  </div>

  <!-- DNN NOTES -->
  <div id="tab-dnn" class="tab-content" id="dnn">
    <div style="margin-bottom:1.5rem;padding:1.25rem;background:var(--peach-soft);border-radius:var(--radius-sm)">
      <strong style="color:var(--accent-orange)">ğŸ’¡ DNN Strategy:</strong>
      <span style="color:var(--text-mid);font-size:0.9rem"> Theory + implementation both matter. Implement backprop from scratch once â€” this cements everything. Know the gate equations for LSTM and the attention formula for Transformers.</span>
    </div>
    <div class="accordion">

      <div class="accordion-item">
        <button class="accordion-header">Activation Functions â€” Properties &amp; When to Use <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <table>
            <thead><tr><th>Function</th><th>Formula</th><th>Derivative</th><th>Range</th><th>Use When</th></tr></thead>
            <tbody>
              <tr><td><strong>Sigmoid</strong></td><td>1/(1+eâ»Ë£)</td><td>Ïƒ(x)(1-Ïƒ(x))</td><td>(0,1)</td><td>Output layer (binary classification)</td></tr>
              <tr><td><strong>Tanh</strong></td><td>(eË£-eâ»Ë£)/(eË£+eâ»Ë£)</td><td>1-tanhÂ²(x)</td><td>(-1,1)</td><td>Hidden layers (zero-centered)</td></tr>
              <tr><td><strong>ReLU</strong></td><td>max(0,x)</td><td>0 if x&lt;0, 1 if x&gt;0</td><td>[0,âˆ)</td><td>Hidden layers (default choice)</td></tr>
              <tr><td><strong>Leaky ReLU</strong></td><td>max(Î±x,x)</td><td>Î± if x&lt;0, 1 if x&gt;0</td><td>(-âˆ,âˆ)</td><td>Prevents dying ReLU</td></tr>
              <tr><td><strong>Softmax</strong></td><td>eË£â±/Î£eË£Ê²</td><td>Complex</td><td>(0,1)</td><td>Output layer (multiclass)</td></tr>
            </tbody>
          </table>
          <p style="margin-top:1rem;color:var(--text-mid);font-size:0.9rem">ğŸ’¡ Sigmoid/Tanh saturate â†’ vanishing gradients. ReLU solves this but can "die" (always output 0). Leaky ReLU &amp; ELU address dying ReLU.</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Backpropagation â€” Step by Step <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Forward Pass:</strong> Compute output layer by layer: zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾, aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾).</p>
          <p style="margin-top:1rem"><strong>Compute Loss:</strong> e.g., Cross-entropy L = -Î£yáµ¢ log(Å·áµ¢).</p>
          <p style="margin-top:1rem"><strong>Backward Pass (output layer):</strong> Î´â½á´¸â¾ = âˆ‚L/âˆ‚zâ½á´¸â¾ = Å· - y (for softmax + cross-entropy).</p>
          <p style="margin-top:1rem"><strong>Backward Pass (hidden layer l):</strong> Î´â½Ë¡â¾ = (Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾ âŠ™ Ïƒ'(zâ½Ë¡â¾)</p>
          <p style="margin-top:1rem"><strong>Weight Updates:</strong> âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(aâ½Ë¡â»Â¹â¾)áµ€, âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾. Then: W â† W - Î±Â·âˆ‚L/âˆ‚W.</p>
          <p style="margin-top:1rem"><strong>Vanishing Gradients:</strong> With sigmoid/tanh, Ïƒ'(z) â‰¤ 0.25. Over L layers, gradients shrink as 0.25á´¸. Solutions: ReLU, BatchNorm, residual connections, LSTM.</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">LSTM â€” All Gate Equations <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>LSTM solves vanishing gradients for sequences via the cell state highway.</strong></p>
          <p style="margin-top:1rem"><strong>Gates (all use sigmoid â†’ output in (0,1)):</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li><strong>Forget gate:</strong> fâ‚œ = Ïƒ(WfÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bf) â€” what to forget from cell state</li>
            <li><strong>Input gate:</strong> iâ‚œ = Ïƒ(Wáµ¢Â·[hâ‚œâ‚‹â‚, xâ‚œ] + báµ¢) â€” what to update</li>
            <li><strong>Candidate:</strong> CÌƒâ‚œ = tanh(WcÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bc) â€” new candidate values</li>
            <li><strong>Output gate:</strong> oâ‚œ = Ïƒ(WoÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bo) â€” what to output</li>
          </ul>
          <p style="margin-top:1rem"><strong>State Updates:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Cell state: Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ</li>
            <li>Hidden state: hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)</li>
          </ul>
          <p style="margin-top:1rem"><strong>Key insight:</strong> The cell state Câ‚œ flows through the network with only element-wise operations, creating a gradient highway that avoids vanishing gradients.</p>
        </div>
      </div>

      <div class="accordion-item">
        <button class="accordion-header">Transformers &amp; Attention Mechanism <span class="acc-icon">â–¼</span></button>
        <div class="accordion-body">
          <p><strong>Attention Formula:</strong> Attention(Q, K, V) = softmax(QKáµ€/âˆšdâ‚–)V</p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Q (Queries): what we're looking for</li>
            <li>K (Keys): what each position has to offer</li>
            <li>V (Values): the actual content to aggregate</li>
            <li>âˆšdâ‚–: scaling to prevent vanishing softmax gradients in high dimensions</li>
          </ul>
          <p style="margin-top:1rem"><strong>Multi-Head Attention:</strong> Run h attention heads in parallel with different learned Q,K,V projections, then concatenate and project. Allows attending to different representation subspaces simultaneously.</p>
          <p style="margin-top:1rem"><strong>Transformer vs RNN:</strong></p>
          <ul style="margin-left:1.5rem;margin-top:0.5rem;color:var(--text-mid)">
            <li>Attention: parallelizable, O(nÂ²) complexity, full context access</li>
            <li>RNN: sequential, O(n) complexity, limited context via hidden state</li>
          </ul>
          <p style="margin-top:1rem"><strong>Positional Encoding:</strong> PE(pos,2i) = sin(pos/10000^(2i/d)), PE(pos,2i+1) = cos(pos/10000^(2i/d)). Injected into embeddings to encode position.</p>
        </div>
      </div>

    </div>
  </div>

</div><!-- end section -->

<!-- Resources Section -->
<div style="background:var(--lavender-soft);padding:3rem 1.5rem">
  <div style="max-width:1200px;margin:0 auto">
    <div class="section-header">
      <h2>Top Free Online Resources</h2>
      <p>Curated from the BITS WILP community â€” 100% free, 100% legitimate.</p>
    </div>
    <ul class="resource-list">
      <li><span class="res-icon">ğŸ“˜</span><div class="res-info"><strong>MML Book (Free PDF)</strong><small>mml-book.github.io â€” Official Deisenroth textbook + exercises + notebooks</small></div></li>
      <li><span class="res-icon">ğŸ“Š</span><div class="res-info"><strong>Think Stats (Free PDF)</strong><small>Allen Downey â€” Practical statistics with Python</small></div></li>
      <li><span class="res-icon">ğŸ¤–</span><div class="res-info"><strong>PRML by Bishop (Free)</strong><small>Microsoft Research official PDF â€” comprehensive ML theory</small></div></li>
      <li><span class="res-icon">ğŸ§ </span><div class="res-info"><strong>Deep Learning Book (Free)</strong><small>deeplearningbook.org â€” Goodfellow, Bengio &amp; Courville official site</small></div></li>
      <li><span class="res-icon">ğŸ’»</span><div class="res-info"><strong>ageron/handson-ml3</strong><small>GitHub â€” Official notebooks for GÃ©ron's book. Run these first for ML &amp; DNN!</small></div></li>
      <li><span class="res-icon">ğŸ¥</span><div class="res-info"><strong>StatQuest YouTube</strong><small>youtube.com/@statquest â€” Statistics &amp; ML explained visually. Superb for ISM concepts.</small></div></li>
      <li><span class="res-icon">ğŸ¥</span><div class="res-info"><strong>3Blue1Brown â€” Essence of Linear Algebra</strong><small>YouTube playlist â€” The best visual introduction to linear algebra. Must-watch for MFML.</small></div></li>
      <li><span class="res-icon">ğŸŒ</span><div class="res-info"><strong>CrackBITSWILP</strong><small>crackbitswilp.in â€” Best WILP community site, PYQs, tips, study guides</small></div></li>
    </ul>
  </div>
</div>

<div class="moti-banner">
  <h2>You already know more than you think you do.</h2>
  <p>Confidence comes from reviewing what you've learned, not from worrying about what you haven't. Keep going. ğŸ’œ</p>
</div>

<footer>
  <div class="footer-inner">
    <div class="footer-brand"><h4>ğŸ“ EC3 Prep Hub</h4><p>Free, community-built prep for BITS WILP AIML. Not affiliated with BITS Pilani.</p></div>
    <div><h4>Quick Links</h4><ul>
      <li><a href="../index.html">Home</a></li>
      <li><a href="mocktest.html">Mock Tests</a></li>
      <li><a href="revision.html">Quick Revision</a></li>
      <li><a href="confidence.html">Confidence Booster</a></li>
    </ul></div>
  </div>
  <div class="footer-bottom"><p>Disclaimer: Independent student resource. Not affiliated with BITS Pilani. Notes are original summaries for educational purposes. Verify technical content with official textbooks. Built with ğŸ’œ for the WILP community.</p></div>
</footer>

<script src="../js/main.js"></script>
</body>
</html>
